---
title:  "집단 지성의 힘"
excerpt: "핸즈온 머신러닝 1.7장 내용 정리"

categories:
  - study
tags:
  - Study
  - MachineLearning
  - Dataanalysis

use_math: True
---

# 7장. 앙상블 학습과 랜덤 포레스트

> 앙상블 방법론들과 결정 트리를 활용한 앙상블 모델들
기존에 공부한 내용 제외하고 중요한 내용만 요약하기.

## 앙상블이란?

- 여러 예측기들을 모아 만든 모델
    - 집단지성
    - 예시
        - 결정 트리를 여러개 만들어서 이를 합침
        - 선형 모델, KNN모델, SVM모델 등 다양한 모델의 predict_proba를 평균내어 확률이 높은 값으로 분류
        - 결정 트리를 만들고, 여기서 오차가 발생하는 부분을 개선하여 다음 결정 트리를 만듦
- 주요 방법들
    - 배깅, 부스팅, 스태킹 등이 있음

## 투표 기반 분류기

- 직접 투표 분류기
    - 여러 분류기를 만들고, 각 분류기의 예측을 모아 가장 많이 선택된 클래스로 예측
    - Like 다수결
- 간접 투표 분류기
    - 개별 분류기의 **예측을 평균**내어 확률이 가장 높은 클래스로 분류
        - 여기서 예측을 평균내기 위해, 각 모델들은 predict_proba() 메서드가 존재해야함
        - 여담으로, SVC에서 probability 매개변수를 True로 지정 시 predict_proba() 메서드 사용 가능

## 배깅과 페이스팅

- 배깅과 페이스팅 두 방법 모두훈련 세트의 **서브셋을 무작위로 구성**하여 모델을 학습시키는 방법
    - 어떻게 무작위로 구성하느냐에 있어 차이가 있음
- 배깅(bagging)
    - 훈련세트에서 랜덤 복원 추출 (부트스트랩)
- 페이스팅(pasting)
    - 중복 허용 x
    - 비복원 랜덤 추출
- 일반적으로는 배깅을 더 선호.
    - 다만, 페이스팅은 편향이 배깅에 비해 낮음
    - cpu에 여유 있으면 교차 검증으로 두 모델 비교하는 선택도 좋음

### OOB 평가 (Out Of Bag)

- 부트스트랩을 하면, 전체 샘플에서 안 쓰이는 샘플이 꼭 쓰임
- 이렇게 안 쓰이는 데이터는 평균적으로 30~40% 가량.
    - 이 데이터들을 검증 세트로 활용하면 됨.

## 랜덤 패치와 랜덤 서브스페이스

- 랜덤 패치 방식
    - 피처와 샘플을 모두 샘플링하는 것
- 랜덤 서브스페이스 방식
    - 특성은 샘플링, 피처는 모두 사용하는 것

## 랜덤 포레스트

- 배깅 방식을 적용한 결정 트리의 앙상블
- 하나의 트리를 만들 때, 특성을 임의로 선택하여 무작위성을 더 부여.
- 독립적인 트리들을 만들기 떄문에, 여러 코어 활용 가능

## 부스팅

- 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법
- 인기 있는 모델은 에이다 부스트(AdaBoost)와 그레이디언트 부스팅(gradient boosting)

### 에이다 부스트

- 이전에 만들어진 모델에서 과소적합된 훈련 샘플의 가중치를 더 높임
    - 위와 같은 방식을 지속 수행
    - 결국 이전에 만들어진 모델을 보정하는 모델을 만들어내는 것.

### 그래이디언트 부스팅

- 맨 처음에 만든 모델의 오차에 대해 보정하는 모델을 만들어나감
    - 마치 선형 회귀에서 비용 함수에서 경사하강법을 사용하는 것처럼 오차 함수를 계속 만들어나감
    - 그러면 처음 만든 모델의 오차가 크더라도, 이후 오차가 계속 보정됨.
    - 여기서 이 오차를 얼마나 보정할지 학습률 매개변수를 통해 조절할 수 있음
- 그레이디언트 부스팅는 각 트리가 훈련할 때 사용할 훈련의 샘플의 비율을 조정할 수 있음
    - 예를 들어, subsample=0.25라고 하면 각 트리는 무작위로 선택된 25%의 훈련 샘플로 학습
    - 분산은 낮아지고, 편향은 높아짐.
    - 이를 확률적 그레이디언트 부스팅이라고 하며,
    - XGBoost 라이브러리에서 잘 구현해놓았음. (Python)
